## Motivation
Detecting 3D objects in urban environment is a fundamental and challenging problemfor motion planning in order to plan a safe route in autonomous driving. Specifically, autonomousvehicles (AVs) need to detect and track moving objects such as pedestrians, cyclists and vehicles inrealtime. Computation speed is critical. AVs carry a variety of sensors such as camera and LiDAR(Light Detection and Ranging), etc.  Recent approaches for 3D object detection either fuse RGBimage from camera and point cloud from LiDAR or use point cloud alone. Point cloud is irregularand extremely computational, but it is crucial for accurate 3D estimation compared with 2D images.Therefore, converting and utilizing point cloud data more efficiently and effectively has become theprimary problem in the detection task, which is also quite interesting and challenging for us. 

## Problem Formulation

**Input:** Point cloud of the scene from the LiDAR sensor. The point cloud is a set of points in 3D space which provides a sparse representation of a 3D shape or object. Each point is composed of the 3D coordinates X, Y and Z.

**Output:** 3D oriented bounding boxes for objects in the scene and the corresponding class labels. A 3D oriented box has seven parameters: 3D location of the box center, length, height and width of the box, and the yaw.

**Demo:** In the following demo, we visualize the input point cloud with the ground-truth box (green) and our predictions (red) embedded. Moreover, we project the point cloud into images and show the corresponding 2D view. The data source of this video clip is part of the tracking benchmark of KITTI [[1](http://www.cvlibs.net/datasets/kitti/eval_tracking.php)]. 
<center>

<figure class="video_container">
  <iframe width="700" height="577" src="https://www.youtube.com/embed/fu5a-p7QrDE" frameborder="0" allowfullscreen="true"> </iframe>
</figure>

</center>

## Introduction
Except camera based approaches which utilize either monocular or stereo images, existing methods are divided into two groups based on input data, [[2](https://arxiv.org/abs/1902.06326), [3](https://arxiv.org/abs/1711.06396), [4](https://www.mdpi.com/1424-8220/18/10/3337/htm), [5](https://arxiv.org/abs/1812.05784), [6](https://arxiv.org/abs/2002.10187)] uses point cloudonly, [[7](https://arxiv.org/abs/1611.07759), [8](https://arxiv.org/abs/1711.08488)] fuse point cloud with RGB images. The fused model has nearly linear computation costwith respect to the number of input modalities, making realtime application infeasible. In our method, we will do object detection with LiDAR only.

There are two main streams in dealing with the unstructured point cloud data, voxel-based and point-based methods. Voxel-based methods convert sparse point cloud into compact representations with a regular shape so that it can adopt existing 2D detection methods without extra efforts. The conversion is conducted either by projecting into images [[7](https://arxiv.org/abs/1611.07759)] or dividing into equally distributed voxels [[3](https://arxiv.org/abs/1711.06396), [2](https://arxiv.org/abs/1902.06326), [4](https://www.mdpi.com/1424-8220/18/10/3337/htm), [5](https://arxiv.org/abs/1812.05784)]. Features in each voxel to the backbone 2D CNN are either handcrafted [[2](https://arxiv.org/abs/1902.06326)] or generated by PointNet-like backbones [[9](http://stanford.edu/~rqi/pointnet/), [10](http://stanford.edu/~rqi/pointnet2/)]. Voxel-based methods are straightforward and efficientwhile suffering from information loss and performance bottleneck. Point-based [[9](http://stanford.edu/~rqi/pointnet/), [10](http://stanford.edu/~rqi/pointnet2/), [11](https://arxiv.org/abs/1812.04244)] methodstakes point cloud as input and outputs bounding boxes on each point, which is usually more accuratebut less efficient than voxel-based methods. Set abstraction and feature propagation are two basicmodules for point-based methods, where the former is for downsampling and extracting contextfeatures and the latter is for upsampling and broadcasting features to points. The sampling strategiesare crucial to point-based methods which is an active research field.

Our designed framework is a point-based single-stage object detector. We borrow the ideas from the new lightweight and effective 3DSSD [[6](https://arxiv.org/abs/2002.10187)] that we eliminate the time-consumingFeature Propagation (FP) layers. What is more, another computational expensive multi-scale grouping (MSG) of local PointNet is replaced with the Dilated Residual Block as the new Set Abstraction (SA) module inspired by a state-of-the-art point cloud semantic segmentation framework RandLA-Net [[12](https://arxiv.org/abs/1911.11236)]. We replace the max-pooling in the original 3DSSD SA module with the attentive pooling to focusmore on object points with target semantic meanings. In the prediction head, the bounding box in 3D instead of BEV (2D) is generated at a time and it is anchor-free.

## The Generalization Approach

As a single-stage point-based method, our network will be composed of a backbone for feature learning, and box prediction network which includes a candidate generation layer and an anchor-freeprediction head. The overview of the whole framework is shown in Fig. 1.

<p align="center">
  <img src="/fast-3d-object-detection/doc/pipeline.png" alt='pipeline' width="1000"> <br>
  <em>Figure 1: Architecture Overview. The input point cloud has four dimesions: (x, y, z, r). The output is bounding box and class label.</em>
</p>

### Backbone: Multi-Scale V.S. Multi-Resolution?

The Backbone takes point cloud as input, and outputs the feature representation that is fed intothe box prediction network. Current state-of-the-art point-based methods such as PointRCNN [[11](https://arxiv.org/abs/1812.04244)] usually follow the architecture of PointNet++ [[10](http://stanford.edu/~rqi/pointnet2/)], which uses Set Abstraction (SA) and Feature Propagation (FP) as two basic modules, as backbones. The SA layers downsample points to gradually enlarge receptive field for better efficiency and effectiveness. The FP layers are used to upsample by broadcasting features from kept points to dropped points in the SA module. 3DSSD [[6](https://arxiv.org/abs/2002.10187)] claims that the FP module is time consuming, limiting the efficiency of the whole framework. However, without FP layer, the foreground instances’ points will only occupy 51.8% when the total point number is downsampled to 512, which means half of the points are just background points. Therefore,they design a new sampling method called Feature-FPS (F-FPS), increasing the foreground points’proportion to 76.1%. In order to not affect the classification, we still need background points, therefore they split the points into two groups, one uses the F-FPS, the other uses D-FPS. This is the so-called fusion sampling.

However, the commonly used SA module has one problem: they are usually multi-scale grouping (MSG) which is computational expensive. That is, after downsampling at each stage, we need to find multi-scale nearest neighbors set and then do MLP and max pooling on each set which is called local PointNet [[9](http://stanford.edu/~rqi/pointnet/)]. After that, the features of every scale are concatenated and fed into a 1d convolution layer to obtain a certain number of features. In particular, the number of points is usually quite large at the first stage which costs significant time. Therefore, we are motivated to design a more efficient point-based backbone. When designing the backbone, we are inspired by RandLA-Net [[12](https://arxiv.org/abs/1911.11236)]. The RandLA-Net [[12](https://arxiv.org/abs/1911.11236)] is a semantic segmentation method achieving state-of-the-art performances onlarge-scale point cloud, the key success of this method is its ability to handle millions of point cloud in realtime.  The main change that we conduct in the backbone is that we replace MSG of local PointNet with a Dilated Residual Block (DRB) that is adopted in RandLA-Net [[12](https://arxiv.org/abs/1911.11236)]. The comparison is shown in Fig. 2.

<p align="center">
  <img src="/fast-3d-object-detection/doc/msg.png" alt='comparison 1' width="700"> <br> 
  <em>(a) MSG of Local PointNet</em> <br> <br>
  <img src="/fast-3d-object-detection/doc/dilated.png" alt='comparison 2'> <br>
  <em>(b) Dilated Residual Block</em> <br>
  <em>Figure 2: Comparison between MSG of local PointNet and dilated residual block.</em>
</p> 

First, instead of simply applying MLP layers on the ball queried set (local set), we can do local spatial encoding to explicitly observe the local geometric patterns thus learning complex local structures. For each of the nearest \\(K\\) points \\(\\{p_i^1 \\cdots p_i^k \\cdots p_i^K \\}\\) of center point \\(p_i\\), we encode as follows:
\\[r_i^k = MLP(p_i \\oplus p_i^k \\oplus (p_i - p_i^k)  \oplus ||p_i - p_i^k||)\\]
where \\(p_i\\) are the 3d coordinates. Then, we concatenate \\(r_i^k\\) with \\(f_i^k\\) which is the corresponding point features. In the local PointNet structure, they only concatenate the 3d coordinates with \\(f_i^k\\). 

Second, we do attentive pooling instead of max-pooling. We use a shared MLP followed by softmax to learn a unique attention score for each feature,
\\[s_i^k = g(\\hat f_i^k, W)\\]
where \\(W\\) is the learnable weights. Then we do the weighted sum on the features:
\\[\\tilde f_i^k = \\sum_{k=1}^{K}(\\hat f_i^k \cdot s_i^k)\\]
    
Third, we stack two above process with a skip collection. The stack of above process increases the receptive field which compensates the lack of MSG. 

### Box Prediction Network

The box prediction network includes a candidate generation layer and an anchor-free regression head similar to 3DSSD [[6](https://arxiv.org/abs/2002.10187)]. The candidate generation layer is a variant of SA layer. The points from F-FPS are used as initial center points and shifted under the supervision of relative locations to their instance center becoming candidate points. Then the surrounding points of each candidate point are found from the whole representation point set with a pre-defined range threshold. Next, MLP layers are applied to the concatenation of the normalized locations and semantic features. The extracted feature are fed into the anchor-free regression head. The distance \\((dist_x, dist_y, dist_z)\\) to its corresponding instance as well as the size \\((d_l, d_w, d_h)\\) and orientation of its corresponding instance are predicted for each candidate point in the anchor-free regression head. The orientation angle regression simply follow F-PointNet [[8](https://arxiv.org/abs/1711.08488)] which pre-dedines 12 equally split orientation angle bins. First, the proposal orientation is classified to know which bin itbelongs to and then regressed the residual with respect to the bin value.

### Loss Function
We use the same loss functions introduced in 3DSSD [[6](https://arxiv.org/abs/2002.10187)]. The total loss consists of regression loss, object classification loss, and the shifting loss.

**Regression Loss:**    Four different components are included in the regression loss: the distance loss, the size loss, the angle loss and the corner loss.
Huber loss is used for all the regression part since it is less sensitive to MSE and can prevent potential exploding gradient problem.
Specifically, in the angle prediction, we quantize the orientation space into finite number (\\(N_{angle}\\)) of bins 
and predict the index of bin using classification. 
Then a regression is used to predict the residue between the bin value and the ground truth value 
to compensate for the quantization error. Therefore, the angle regression loss can be expressed as
\\[L_{\theta}=L_{cls_a}(\theta_{b},\hat{\theta_b})+L_{reg_a}(\theta_r,\hat{\theta_{r}})\\]
where \\(\theta_b\\) and \\(\hat{\theta_{b}}\\) represent the ground truth and predicted class for angle bin, 
and \\(\theta_r\\) and \\(\hat{\theta_{r}}\\) represent the ground truth and regressed residue values.

However, only with the predictions of center, size and angle is not enough for an accurate prediction 
of the final box because they are calculated in three separated terms. 
Therefore, the corner loss is used to combine them together, since the position of the eight corners 
of a box is determined by center, size and angle jointly. It can be expressed as

\\[L_{corner}=\displaystyle\sum\limits_{k=1}^8 \parallel cor_{k}-\hat{cor_{k}} \parallel\\]

where \\(cor_{k}\\) and \\(\hat{cor_{k}}\\) represent ground truth and the predicted location of the k-th corner.

**Object Classification Loss:**     For object class loss \\(L_{cls}\\), the ground truth labels need to be distributed 
to every single point in the point cloud data. The center-ness score is used to put more 
weights on the candidate points closer to the center of an instance after the shifting. 
The classification loss is then calculated using the cross entropy loss.

**Shifting Loss:**      The shifting loss Lshift corresponds to the supervised shifting operation in the 
candidate generation layer. We use Huber loss to reduce the distance between 
the predicted shifts and the residue from candidate points to the instance centers.

Finally, the total loss is the weighted summation of these three loss sources. Here we need to note that, different 
subsets of points contribute to different loss terms. The regression loss \\(L_{reg}\\) is computed based on the 
\\(N_{pc}\\) positive candidate points, while the object classification loss \\(L_{cls}\\) is from all the \\(N_{c}\\) candidate points. 
For the shifting loss, since we adopted both F-FPS and D-FPS, we only count on the \\(N_{p}^\ast\\) positive representative points from F-FPS.

\\[L_{total}=\frac{1}{N_{pc}}\displaystyle\sum\limits_{i}^{N_{pc}} L_{reg}^i+
\gamma\frac{1}{N_{c}}\displaystyle\sum\limits_{i}^{N_{c}} L_{cls}^i+
\beta\frac{1}{N_{p}^\ast}\displaystyle\sum\limits_{i}^{N_{p}^\ast} L_{shift}^i\\]

## Experiments

### Dataset ###
We use the KITTI object detection benchmark dataset [[13](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)]. It has 7481 training point clouds and 7518 testing point clouds, comprising 80256 labeled objects in total. There are mainly three types of objects: car, pedestrian and cyclist. Since the ground truth label of the testing set is not publicly available, we will follow previous papers to split the training set into 3712 training samples and 3769 validation samples. We train our model on the training samples and evaluate the performance on the validation set.

### Quantitative Results ###
Table 1 summarizes the quantitative results evaluated on the 3D validation set for "Car". 
Our Average Precision (AP) is very close to the PointRCNN [[11](https://arxiv.org/abs/1812.04244)] which is 2-stage in the Easy mode, 
and is competitive in Moderate and Hard with the state-of-the-art 1-stage framework 3DSSD [[6](https://arxiv.org/abs/2002.10187)]. 
Notably, we can achieve faster training and inference time, which is of vital significance for our real-time scenario in order to give an instant feedback of the environment condition in the self-driving application. For example, for a 10Hz autonomous driving system, the distance covered without perception during the inference time is around 30m/s * 0.1s = 3m, which is even longer than a mini car.

| Methods | Easy AP |Moderate AP|Hard AP |Inference Time (fps) |
| :---:   |   :---:   | :---:   | :---:   | :---:   |
|Ours - 1|86.32|73.43|70.66|36|
|Ours - 2|87.16|74.12|71.22|35|
|Ours - 3|87.44|76.11|71.66|26|
|Ours - 4|87.65|76.97|74.24|28|
|PointRCNN|88.91|79.88|78.37|-|
|3DSSD|91.43|82.93|79.99|26|

<p align="center"></p>
<table>
    <thead>
        <tr>
            <th>Methods</th>
            <th>Easy AP</th>
            <th>Moderate AP</th>
            <th>Hard AP</th>
            <th>Inference Time (fps)</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Ours - 1</td>
            <td>86.32</td>
            <td>73.43</td>
            <td>70.66</td>
            <td>36</td>
        </tr>
        <tr>
            <td>Ours - 2</td>
            <td>87.16</td>
            <td>74.12</td>
            <td>71.22</td>
            <td>35</td>
        </tr>
        <tr>
            <td>Ours - 3</td>
            <td>87.44</td>
            <td>76.11</td>
            <td>71.66</td>
            <td>26</td>
        </tr>
        <tr>
            <td>Ours - 4</td>
            <td>87.65</td>
            <td>76.97</td>
            <td>74.24</td>
            <td>28</td>
        </tr>       
        <tr>
            <td>PointRCNN</td>
            <td>88.91</td>
            <td>79.88</td>
            <td>78.37</td>
            <td> - </td>
        </tr>
        <tr>
            <td>3DSSD</td>
            <td>91.43</td>
            <td>82.93</td>
            <td>79.99</td>
            <td>26</td>
        </tr> 
    </tbody>
</table>
<p></p>


### Qualitative Results ###
Figure 3 shows visualization on several samples. The detector is able to catch all ground truth cars in most scenes, though sometimes there are some predictions in unlabeled area. Note that not all of those are false alarms, some of those cars are not labeled because they are far from the lidar center.

<p align="center">
  <img src="/fast-3d-object-detection/doc/viz_1.png"> <br>
  <em>Figure 3: Visualize results on point cloud. Different rows show different views. The green boxes represent the  ground truth, while the red ones are our predictions.</em>
</p>

Figure 4 compares our model with the state of art 3DSSD. 3DSSD seems to make more proposals so that some difficult cases such as large occlusion can be captured, but it also introduces some obvious false alarms, while our results are quite clean. In this example, 3DSSD suggests that a shallow tree is a car, which is not correct.

<p align="center">
  <img src="/fast-3d-object-detection/doc/viz_2.png"> <br>
  <em>Figure 4: Comparison between 3DSSD and our model. The left side is for 3DSSD while the right side is ours.</em>
</p>


## What's Next?
In this project, we only did some fundamental researches on the backbone design, trying to use a DRB rather than the time consuming MSG. There are many other exciting research areas except backbone design in 3D object detection. For example, the sampling method is an active research topic. FPS is usually adopted in a point-based method, however, the time complexity is \\(O(n^2)\\) which is unrealistic in a realtime application. Some deep learning based sampling methods have been proposed recently, but these methods can't generalize well so far and usually takes longer. Some researchers also try random sampling which is \\(O(1)\\), but it is not as effective as FPS. More researches will be conducted on those interesting areas.

