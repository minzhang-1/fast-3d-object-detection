## Motivation
Detecting 3D objects in urban environment is a fundamental and challenging problemfor motion planning in order to plan a safe route in autonomous driving. Specifically, autonomousvehicles (AVs) need to detect and track moving objects such as pedestrians, cyclists and vehicles inrealtime. Computation speed is critical. AVs carry a variety of sensors such as camera and LiDAR(Light Detection and Ranging), etc.  Recent approaches for 3D object detection either fuse RGBimage from camera and point cloud from LiDAR or use point cloud alone. Point cloud is irregularand extremely computational, but it is crucial for accurate 3D estimation compared with 2D images.Therefore, converting and utilizing point cloud data more efficiently and effectively has become theprimary problem in the detection task, which is also quite interesting and challenging for us. 

## Problem Formulation

**Input:**    point cloud of the scene from the LiDAR sensor. The point cloud is a set of points in 3D space which provides a sparse representation of a 3D shape or object. Each point is composed of the 3D coordinates X, Y and Z.

**Output:**    3D oriented bounding boxes for objects in the scene and the corresponding class labels. A 3D oriented box has seven parameters: 3D location of the box center, length, height and width of the box, and the yaw.

**Demo:** In the following demo, we visualize the input point cloud and show the corresponding 2D view, with the ground-truth box (green) and our predictions (red) embedded. The data source of this video clip is part of the tracking benchmark of KITTI.

<figure class="video_container">
  <iframe src="https://youtu.be/f9Yqx8WpKfk" frameborder="0" allowfullscreen="true"> </iframe>
</figure>


## Background Introduction
Except camera based approaches which utilize either monocular or stereo images,existing methods are divided into two groups based on input data, [[1](https://arxiv.org/abs/1902.06326), [2](https://arxiv.org/abs/1711.06396), [3](https://www.mdpi.com/1424-8220/18/10/3337/htm), [4](https://arxiv.org/abs/1812.05784), [5](https://arxiv.org/abs/2002.10187)] uses point cloudonly, [[6](https://arxiv.org/abs/1611.07759), [7](https://arxiv.org/abs/1711.08488)] fuse point cloud with RGB images. The fused model has nearly linear computation costwith respect to the number of input modalities, making realtime application infeasible. In our method,we will do object detection with LiDAR only.

There are two main streams in dealing with the unstructured point cloud data, voxel-based andpoint-based methods. Voxel-based methods convert sparse point cloud into compact representationswith a regular shape so that it can adopt existing 2D detection methods without extra efforts. Theconversion is conducted either by projecting into images [2] or dividing into equally distributedvoxels [15,13,12,7]. Features in each voxel to the backbone 2D CNN are either handcrafted [13] orgenerated by PointNet-like backbones [8,10]. Voxel-based methods are straightforward and efficientwhile suffering from information loss and performance bottleneck. Point-based [8, 10, 11] methodstakes point cloud as input and outputs bounding boxes on each point, which is usually more accuratebut less efficient than voxel-based methods. Set abstraction and feature propagation are two basicmodules for point-based methods, where the former is for downsampling and extracting contextfeatures and the latter is for upsampling and broadcasting features to points. The sampling strategiesare crucial to point-based methods which is an active research field.

## The Generalization Approach

Overview of the architecture with figure. ZM

<p align="center">
  <img src="/fast-3d-object-detection/doc/pipeline.png" alt='pipeline'> <br>
  <em>Figure 1: Architecture Overview. The input point cloud has four dimesions: (x, y, z, r). The output is bounding box and class label.</em>
</p>

### Backbone: Multi-Scale V.S. Multi-Resolution? ZM

Illustration and figure.

<p align="center">
  <img src="/fast-3d-object-detection/doc/msg.png" alt='comparison 1'> <br>
  <em>(a) MSG of Local PointNet</em> <br>
  <img src="/fast-3d-object-detection/doc/dilated.png" alt='comparison 2'> <br>
  <em>(b) Dilated Residual Block</em> <br>
  <em>Figure 2: Comparison between MSG of local PointNet and dilated residual block.</em>
</p>

### Box Prediction Network  ZM

### Loss Function  YJ

## Experiments

### Quantitative Results ###
YJ table

### Qualitative Results ###
Figure 3 shows visualization on several samples. The detector is able to catch all ground truth cars in most scenes, though sometimes there are some predictions in unlabeled area. Note that not all of those are false alarms, some of those cars are not labeled because they are far from the lidar center.

<p align="center">
  <img src="/fast-3d-object-detection/doc/viz_1.png"> <br>
  <em>Figure 3: Visualize results on point cloud. Different rows show different views. The green boxes represent the  ground truth, while the red ones are our predictions.</em>
</p>

## References


### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).
